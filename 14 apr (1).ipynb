{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc3096f-93ef-42d9-9e02-af7e5c1641eb",
   "metadata": {},
   "source": [
    "## Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea078132-e046-4505-98b3-412a51600670",
   "metadata": {},
   "source": [
    "##    \n",
    "1)Handling Missing Values:\n",
    "*Identify the columns with missing values in the dataset.\n",
    "*Decide on a strategy to handle missing values, such as imputation (filling missing values with a reasonable estimate) or removal of rows/columns with missing values.\n",
    "*For numerical features, common imputation methods include mean, median, or using machine learning algorithms to predict missing values based on other features.\n",
    "*For categorical features, you can either add a new category to represent missing values or use the most frequent category.\n",
    "Encoding Categorical Variables:\n",
    "\n",
    "2)Categorical variables need to be converted into numerical format so that machine learning algorithms can work with them.\n",
    "*One-hot encoding: For nominal categorical variables (categories with no ordinal relationship), you can use one-hot encoding to create binary columns for each category, where 1 indicates the presence of that category and 0 indicates the absence.\n",
    "*Label encoding: For ordinal categorical variables (categories with a specific order), you can use label encoding to map categories to integers.\n",
    "\n",
    "3)Scaling Numerical Features:\n",
    "If your numerical features have different scales, it's often beneficial to scale them to a similar range.\n",
    "Common scaling methods include Min-Max scaling (scaling features to a specified range, usually [0, 1]) and Standardization (scaling features to have mean=0 and standard deviation=1).\n",
    "Scaling is not always necessary for all machine learning algorithms, but it can improve the performance of some algorithms that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d3fb0d-6da3-401e-8b17-a58d43ee506e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, StandardScaler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming 'data' is your dataset loaded into a pandas DataFrame.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Handling Missing Values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Identify columns with missing values\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m columns_with_missing_values \u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns[data\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many()]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Decide on a strategy for imputation (e.g., using mean for numerical features)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Assuming 'data' is your dataset loaded into a pandas DataFrame.\n",
    "\n",
    "# Step 1: Handling Missing Values\n",
    "# Identify columns with missing values\n",
    "columns_with_missing_values =data.columns[data.isnull().any()].tolist()\n",
    "\n",
    "# Decide on a strategy for imputation (e.g., using mean for numerical features)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[columns_with_missing_values] = imputer.fit_transform(data[columns_with_missing_values])\n",
    "\n",
    "# Step 2: Encoding Categorical Variables\n",
    "# Identify categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "encoder = OneHotEncoder()\n",
    "encoded_features = encoder.fit_transform(data[categorical_columns]).toarray()\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names(categorical_columns))\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "data.drop(columns=categorical_columns, inplace=True)\n",
    "\n",
    "# Step 3: Scaling Numerical Features (if necessary)\n",
    "# Identify numerical columns\n",
    "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Standardization for numerical columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Now, the 'data' DataFrame should be preprocessed and ready for use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a831fb-0a24-447f-8d3e-af176f633d0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q2. Split the dataset into a training set (70%) and a test set (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64e1f7-5ee5-4081-ab7a-e2989a37323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your preprocessed DataFrame and 'target' is the target variable column\n",
    "\n",
    "# Splitting the dataset into training (70%) and test (30%) sets\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    data.drop(columns=['target']),  # Drop the target variable column from the features\n",
    "    data['target'],  # Target variable column\n",
    "    test_size=0.3,   # Proportion of test set (30%)\n",
    "    random_state=42  # Setting a random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Now, you have the following datasets ready:\n",
    "# train_data: Training set features (70% of the original data)\n",
    "# test_data: Test set features (30% of the original data)\n",
    "# train_target: Training set target variable\n",
    "# test_target: Test set target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed69ba6-4f1d-4bbc-80b1-b9f054901e2c",
   "metadata": {},
   "source": [
    "## Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17b8ab-5911-4d93-924f-19b741a8dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming you have already split the dataset into train_data, test_data, train_target, and test_target\n",
    "\n",
    "# Create the RandomForestClassifier with the specified hyperparameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_classifier.fit(train_data, train_target)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = rf_classifier.predict(test_data)\n",
    "\n",
    "# Now, you have trained the random forest classifier and made predictions on the test set.\n",
    "# You can proceed to evaluate the performance of the model using various metrics, such as accuracy, precision, recall, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce51e5-4704-4f6e-b792-0c90d7aad954",
   "metadata": {},
   "source": [
    "## Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d947546-d45e-4cff-a643-2c7df7cc37f7",
   "metadata": {},
   "source": [
    "## To evaluate the performance of the trained random forest classifier on the test set using accuracy, precision, recall, and F1 score, you can use the metrics available in the scikit-learn library in Python. Here's how you can calculate these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77076a01-3a14-407f-87ff-878574233225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have already trained the random forest classifier and made predictions on the test set\n",
    "# rf_classifier: Trained RandomForestClassifier\n",
    "# test_data: Test set features\n",
    "# test_target: True labels for the test set\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_classifier.predict(test_data)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_target, predictions)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(test_target, predictions)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(test_target, predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(test_target, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34cd46-85ab-4793-b5dc-529d08ad47f6",
   "metadata": {},
   "source": [
    "##\n",
    "The code above will print the values of accuracy, precision, recall, and F1 score for the random forest classifier's performance on the test set. These metrics provide insights into different aspects of the model's performance:\n",
    "\n",
    "Accuracy measures the proportion of correctly classified instances out of all instances in the test set.\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "Recall (also known as sensitivity) measures the proportion of true positive predictions out of all actual positive instances in the test set.\n",
    "F1 score is the harmonic mean of precision and recall, providing a balanced evaluation metric for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea4458-45a0-4684-9c84-cc928f10c6d2",
   "metadata": {},
   "source": [
    "## Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fb592-c9d7-4ed1-be16-2d63e0428963",
   "metadata": {},
   "source": [
    "## \n",
    "Let's identify the top 5 most important features in predicting heart disease risk using the feature importances from the random forest classifier and visualize them with a bar chart. Here's the correct way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e301ee1-8f26-43c1-83a2-204a462a1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already trained the random forest classifier and stored it in rf_classifier\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate feature names with their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': train_data.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 5 most important features\n",
    "top_5_features = feature_importance_df.head(5)\n",
    "\n",
    "# Visualize the feature importances using a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_5_features['Feature'], top_5_features['Importance'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 5 Most Important Features for Heart Disease Risk Prediction')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a724acf-b50a-438c-83b0-70d61449b3b9",
   "metadata": {},
   "source": [
    "## \n",
    "In this code, we are assuming that train_data is the DataFrame containing the features used for training the model. The train_data.columns attribute provides the feature names. We then create a DataFrame called feature_importance_df to associate each feature's name with its corresponding importance score. We sort this DataFrame in descending order based on importance and select the top 5 features. Finally, we visualize the top 5 most important features using a bar chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4500cf2-34c0-42ee-9bc6-c4cb97da152e",
   "metadata": {},
   "source": [
    "## Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495fd21-649c-43b0-b2c5-07f52ab8025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Assuming you have already split the dataset into train_data, test_data, train_target, and test_target\n",
    "\n",
    "# Create the RandomForestClassifier with default hyperparameters\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid or distribution for the search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],     # Different values for the number of trees\n",
    "    'max_depth': [5, 10, 15],           # Different values for the maximum depth\n",
    "    'min_samples_split': [2, 5, 10],    # Different values for minimum samples split\n",
    "    'min_samples_leaf': [1, 2, 4]       # Different values for minimum samples leaf\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV or RandomizedSearchCV object\n",
    "# For Grid Search:\n",
    "# search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)\n",
    "\n",
    "# For Random Search:\n",
    "search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Perform the search and fit the model to find the best hyperparameters\n",
    "search.fit(train_data, train_target)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding best model\n",
    "best_params = search.best_params_\n",
    "best_rf_classifier = search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_predictions = best_rf_classifier.predict(test_data)\n",
    "\n",
    "# Calculate the performance metrics on the test set\n",
    "accuracy = accuracy_score(test_target, best_predictions)\n",
    "precision = precision_score(test_target, best_predictions)\n",
    "recall = recall_score(test_target, best_predictions)\n",
    "f1 = f1_score(test_target, best_predictions)\n",
    "\n",
    "# Print the evaluation metrics and the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2666c-22bd-4cbb-be52-0a3acb98027c",
   "metadata": {},
   "source": [
    "## Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19430c-b3a3-4a44-a2e2-12b17ce6bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have already trained the default random forest classifier and stored it in rf_classifier\n",
    "# Also, you have performed RandomizedSearchCV and obtained the best hyperparameters and the best model\n",
    "\n",
    "# Evaluate the default model on the test set\n",
    "default_predictions = rf_classifier.predict(test_data)\n",
    "default_accuracy = accuracy_score(test_target, default_predictions)\n",
    "default_precision = precision_score(test_target, default_predictions)\n",
    "default_recall = recall_score(test_target, default_predictions)\n",
    "default_f1 = f1_score(test_target, default_predictions)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_predictions = best_rf_classifier.predict(test_data)\n",
    "best_accuracy = accuracy_score(test_target, best_predictions)\n",
    "best_precision = precision_score(test_target, best_predictions)\n",
    "best_recall = recall_score(test_target, best_predictions)\n",
    "best_f1 = f1_score(test_target, best_predictions)\n",
    "\n",
    "# Print the evaluation metrics and the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "print(\"Default Model Performance:\")\n",
    "print(\"Accuracy:\", default_accuracy)\n",
    "print(\"Precision:\", default_precision)\n",
    "print(\"Recall:\", default_recall)\n",
    "print(\"F1 Score:\", default_f1)\n",
    "\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "print(\"Accuracy:\", best_accuracy)\n",
    "print(\"Precision:\", best_precision)\n",
    "print(\"Recall:\", best_recall)\n",
    "print(\"F1 Score:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc13f5-7990-482d-8c44-2bf05a4eed04",
   "metadata": {},
   "source": [
    "## Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d08a2-3760-4652-a8d9-d370a454f343",
   "metadata": {},
   "source": [
    "## \n",
    "Interpreting the decision boundaries of a random forest classifier can provide insights into how the model makes predictions. However, since random forests are ensemble models composed of multiple decision trees, visualizing the decision boundaries directly can be challenging. Instead, we can use the model's predictions to create a contour plot that approximates the decision boundaries on a scatter plot of two of the most important features.\n",
    "\n",
    "Let's assume you have identified the two most important features based on the feature importances obtained during the model training. For simplicity, we'll use these two features to create the scatter plot and overlay the decision boundaries. Keep in mind that decision boundaries in higher-dimensional spaces can be more complex and may not be accurately represented in two-dimensional scatter plots.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f71c69-5259-4d66-8180-5a7500e4ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have already trained the random forest classifier and identified the two most important features\n",
    "# Let's assume the two most important features are 'feature1' and 'feature2'\n",
    "\n",
    "# Generate data points for the scatter plot within the range of the two features\n",
    "# You can modify this based on your data distribution\n",
    "feature1_range = np.linspace(train_data['feature1'].min(), train_data['feature1'].max(), 100)\n",
    "feature2_range = np.linspace(train_data['feature2'].min(), train_data['feature2'].max(), 100)\n",
    "feature1_values, feature2_values = np.meshgrid(feature1_range, feature2_range)\n",
    "\n",
    "# Create input data for the model by combining the two features\n",
    "input_data = np.column_stack((feature1_values.ravel(), feature2_values.ravel()))\n",
    "\n",
    "# Get the model predictions for the input data\n",
    "predictions = best_rf_classifier.predict(input_data)\n",
    "\n",
    "# Reshape the predictions to the shape of the meshgrid\n",
    "predictions = predictions.reshape(feature1_values.shape)\n",
    "\n",
    "# Plot the decision boundaries on the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(train_data['feature1'], train_data['feature2'], c=train_target, cmap='coolwarm', alpha=0.6)\n",
    "plt.contourf(feature1_values, feature2_values, predictions, alpha=0.3, cmap='coolwarm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.colorbar(label='Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070dfa5-930b-47fe-8dfb-89dba6311eaf",
   "metadata": {},
   "source": [
    "## \n",
    "Interpreting the insights and limitations of the model for predicting heart disease risk from this visualization can be challenging. Some insights that can be derived include:\n",
    "\n",
    "The decision boundaries provide a rough indication of how the model separates different classes based on the two most important features.\n",
    "The regions enclosed by the same decision boundary color represent areas where the model predicts the same class.\n",
    "Decision boundaries are non-linear due to the random forest's ensemble nature, allowing the model to capture complex relationships between features and the target variable.\n",
    "However, it's important to note some limitations:\n",
    "\n",
    "This visualization only shows the decision boundaries for two features, while the model's decision-making involves multiple features. The full decision boundaries in higher-dimensional feature spaces can be much more complex.\n",
    "The model's performance is not solely determined by these two features. Other features might also play critical roles in predicting heart disease risk.\n",
    "The data distribution in the plot may not be representative of the entire dataset, and the visualization does not capture the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70cb50-5c74-4038-a720-953a95427de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
